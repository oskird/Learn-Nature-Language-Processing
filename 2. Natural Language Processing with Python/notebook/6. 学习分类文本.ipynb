{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 学习分类文本\n",
    "1. 我们怎样才能识别语言数据中能明显用于对其分类的特征？\n",
    "2. 我们怎样才能构建语言模型，用于自动执行语言处理任务？\n",
    "3. 从这些模型中我们可以学到哪些关于语言的知识？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 有监督分类\n",
    "- 邮件是否为垃圾邮件\n",
    "- 文章的主题是什么\n",
    "- 某个词语的具体语义是什么"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 性别鉴定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male\n",
      "female\n",
      "the accuracy of train set 0.7619559376679205\n",
      "the accuracy of test set 0.776\n",
      "Most Informative Features\n",
      "             last_letter = 'a'            female : male   =     37.0 : 1.0\n",
      "             last_letter = 'k'              male : female =     31.9 : 1.0\n",
      "             last_letter = 'f'              male : female =     16.5 : 1.0\n",
      "             last_letter = 'p'              male : female =     11.8 : 1.0\n",
      "             last_letter = 'v'              male : female =     10.4 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import names\n",
    "def gender_features(word):\n",
    "    return {'last_letter': word[-1]} # 提取特征，名字最后一个字母\n",
    "labeled_names = ([(name, 'male') for name in names.words('male.txt')] \n",
    "                 + [(name, 'female') for name in names.words('female.txt')]) # 获得有标签的姓名数据\n",
    "import random\n",
    "random.shuffle(labeled_names) # 打乱数据顺序\n",
    "featuresets = [(gender_features(n), g) for (n,g) in labeled_names] # 获取姓名的尾字母特征\n",
    "train_set, test_set = featuresets[500:], featuresets[:500] # 划分训练和测试集\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set) # 训练分类器\n",
    "print(classifier.classify(gender_features('Neo')))\n",
    "print(classifier.classify(gender_features('Trinity')))\n",
    "print('the accuracy of train set {}'.format(nltk.classify.accuracy(classifier, train_set))) # 查看训练集准确率\n",
    "print('the accuracy of test set {}'.format(nltk.classify.accuracy(classifier, test_set))) # 查看测试集准确率\n",
    "print(classifier.show_most_informative_features(5)) # 查看最有用的几个特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 选择正确的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy of train set 0.7760612573885008\n",
      "the accuracy of test set 0.796\n",
      "Most Informative Features\n",
      "             last_letter = 'a'            female : male   =     37.0 : 1.0\n",
      "             last_letter = 'k'              male : female =     31.9 : 1.0\n",
      "             last_letter = 'f'              male : female =     16.5 : 1.0\n",
      "             last_letter = 'p'              male : female =     11.8 : 1.0\n",
      "             last_letter = 'v'              male : female =     10.4 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 添加更多特征\n",
    "def gender_features2(name):\n",
    "    features = {}\n",
    "    features[\"first_letter\"] = name[0].lower() # 首字母\n",
    "    features[\"last_letter\"] = name[-1].lower() # 尾字母\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features[\"count({})\".format(letter)] = name.lower().count(letter) # 名字中某个字母的个数\n",
    "        features[\"has({})\".format(letter)] = (letter in name.lower()) # 名字中是否包含某字母\n",
    "    return features\n",
    "featuresets = [(gender_features2(n), g) for (n,g) in labeled_names] # 提取特征\n",
    "train_set, test_set = featuresets[500:], featuresets[:500] # 划分训练和测试集\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set) # 训练分类器\n",
    "print('the accuracy of train set {}'.format(nltk.classify.accuracy(classifier, train_set))) # 查看训练集准确率\n",
    "print('the accuracy of test set {}'.format(nltk.classify.accuracy(classifier, test_set))) # 查看测试集准确率\n",
    "print(classifier.show_most_informative_features(5)) # 查看最有用的几个特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.749\n"
     ]
    }
   ],
   "source": [
    "# 训练集、开发集和测试集\n",
    "train_names = labeled_names[1500:]\n",
    "devtest_names = labeled_names[500:1500]\n",
    "test_names = labeled_names[:500]\n",
    "train_set = [(gender_features(n), gender) for (n, gender) in train_names]\n",
    "devtest_set = [(gender_features(n), gender) for (n, gender) in devtest_names]\n",
    "test_set = [(gender_features(n), gender) for (n, gender) in test_names]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, devtest_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct=female   guess=male     name=Adrian                        \n",
      "correct=female   guess=male     name=Aidan                         \n",
      "correct=female   guess=male     name=Aigneis                       \n",
      "correct=female   guess=male     name=Allis                         \n",
      "correct=female   guess=male     name=Allsun                        \n",
      "correct=female   guess=male     name=Allyson                       \n",
      "correct=female   guess=male     name=Anett                         \n",
      "correct=female   guess=male     name=Ann                           \n",
      "correct=female   guess=male     name=Aryn                          \n",
      "correct=female   guess=male     name=Ashlen                        \n"
     ]
    }
   ],
   "source": [
    "# 错误分析：查看具体错误\n",
    "errors = []\n",
    "for (name, tag) in devtest_names:\n",
    "    guess = classifier.classify(gender_features(name))\n",
    "    if guess != tag:\n",
    "        errors.append((tag, guess, name))\n",
    "for (tag, guess, name) in sorted(errors)[:10]:\n",
    "    print('correct={:<8} guess={:<8s} name={:<30}'.format(tag, guess, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.777\n"
     ]
    }
   ],
   "source": [
    "# 改进特征\n",
    "def gender_features(word):\n",
    "    return {'suffix1': word[-1:], 'suffix2': word[-2:]}\n",
    "train_set = [(gender_features(n), gender) for (n, gender) in train_names]\n",
    "devtest_set = [(gender_features(n), gender) for (n, gender) in devtest_names]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, devtest_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文档分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取文本和标签\n",
    "from nltk.corpus import movie_reviews\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "              for category in movie_reviews.categories()\n",
    "              for fileid in movie_reviews.fileids(category)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy of train set 0.8752631578947369\n",
      "the accuracy of test set 0.78\n",
      "Most Informative Features\n",
      "    contains(recognizes) = True              pos : neg    =      8.1 : 1.0\n",
      " contains(unimaginative) = True              neg : pos    =      7.8 : 1.0\n",
      "    contains(schumacher) = True              neg : pos    =      7.8 : 1.0\n",
      "        contains(turkey) = True              neg : pos    =      6.5 : 1.0\n",
      "     contains(atrocious) = True              neg : pos    =      6.4 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 统计词频\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "word_features = list(all_words)[:2000] # 提取前2000个词\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words) # 特征：是否包含某一个词\n",
    "    return features\n",
    "\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print('the accuracy of train set {}'.format(nltk.classify.accuracy(classifier, train_set))) # 查看训练集准确率\n",
    "print('the accuracy of test set {}'.format(nltk.classify.accuracy(classifier, test_set))) # 查看测试集准确率\n",
    "print(classifier.show_most_informative_features(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词性标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', ',', '.', 's', 'd', 't', 'he', 'n', 'a', 'of', 'the', 'y', 'r', 'to', 'in', 'f', 'o', 'ed', 'nd', 'is', 'on', 'l', 'g', 'and', 'ng', 'er', 'as', 'ing', 'h', 'at', 'es', 'or', 're', 'it', '``', 'an', \"''\", 'm', ';', 'i', 'ly', 'ion', 'en', 'al', '?', 'nt', 'be', 'hat', 'st', 'his', 'th', 'll', 'le', 'ce', 'by', 'ts', 'me', 've', \"'\", 'se', 'ut', 'was', 'for', 'ent', 'ch', 'k', 'w', 'ld', '`', 'rs', 'ted', 'ere', 'her', 'ne', 'ns', 'ith', 'ad', 'ry', ')', '(', 'te', '--', 'ay', 'ty', 'ot', 'p', 'nce', \"'s\", 'ter', 'om', 'ss', ':', 'we', 'are', 'c', 'ers', 'uld', 'had', 'so', 'ey']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "suffix_fdist = nltk.FreqDist()\n",
    "for word in brown.words():\n",
    "    word = word.lower()\n",
    "    suffix_fdist[word[-1:]] += 1\n",
    "    suffix_fdist[word[-2:]] += 1\n",
    "    suffix_fdist[word[-3:]] += 1\n",
    "\n",
    "common_suffixes = [suffix for (suffix, count) in suffix_fdist.most_common(100)]\n",
    "print(common_suffixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy of train set 0.7133333333333334\n",
      "the accuracy of test set 0.64\n"
     ]
    }
   ],
   "source": [
    "def pos_features(word):\n",
    "    features = {}\n",
    "    for suffix in common_suffixes:\n",
    "        features['endswith({})'.format(suffix)] = word.lower().endswith(suffix)\n",
    "    return features\n",
    "tagged_words = brown.tagged_words(categories='news')[:1000]\n",
    "featuresets = [(pos_features(n), g) for (n,g) in tagged_words]\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.DecisionTreeClassifier.train(train_set) # 使用决策树分类器\n",
    "print('the accuracy of train set {}'.format(nltk.classify.accuracy(classifier, train_set)))\n",
    "print('the accuracy of test set {}'.format(nltk.classify.accuracy(classifier, test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if endswith(the) == False: \n",
      "  if endswith(s) == False: \n",
      "    if endswith(.) == False: \n",
      "      if endswith(,) == False: return 'CD'\n",
      "      if endswith(,) == True: return ','\n",
      "    if endswith(.) == True: return '.'\n",
      "  if endswith(s) == True: \n",
      "    if endswith(as) == False: \n",
      "      if endswith('s) == False: return 'NPS'\n",
      "      if endswith('s) == True: return 'NN$'\n",
      "    if endswith(as) == True: \n",
      "      if endswith(was) == False: return 'CS'\n",
      "      if endswith(was) == True: return 'BEDZ'\n",
      "if endswith(the) == True: return 'AT'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 查看决策树分类逻辑\n",
    "print(classifier.pseudocode(depth=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 探索上下文语境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'suffix(1)': 'n', 'suffix(2)': 'on', 'suffix(3)': 'ion', 'prev-word': 'an'}\n",
      "the accuracy of train set 0.820450885668277\n",
      "the accuracy of test set 0.7947439963751699\n"
     ]
    }
   ],
   "source": [
    "def pos_features(sentence, i):\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]}\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1] # 特征表示当前词的前一个词\n",
    "    return features\n",
    "\n",
    "print(pos_features(brown.sents()[0], 8))\n",
    "\n",
    "tagged_sents = brown.tagged_sents(categories='news')[:1000]\n",
    "featuresets = []\n",
    "for tagged_sent in tagged_sents:\n",
    "    untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "    for i, (word, tag) in enumerate(tagged_sent):\n",
    "        featuresets.append((pos_features(untagged_sent, i), tag))\n",
    "\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "print('the accuracy of train set {}'.format(nltk.classify.accuracy(classifier, train_set)))\n",
    "print('the accuracy of test set {}'.format(nltk.classify.accuracy(classifier, test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 序列分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy of train set 0.8231285649386705\n",
      "the accuracy of test set 0.798941798941799\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "def pos_features(sentence, i, history):\n",
    "    features = {\"suffix(1)\": sentence[i][-1:], # 特征：单词的尾字母\n",
    "                 \"suffix(2)\": sentence[i][-2:], # 特征：单词结尾两个字母\n",
    "                 \"suffix(3)\": sentence[i][-3:]} # 特征：单词结尾三个字母\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\" # 特征：单词的前一个词，如果为局首词标注<START>\n",
    "        features[\"prev-tag\"] = \"<START>\"  # 特征：单词的前一个词的词性，如果为局首词标注<START>\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "        features[\"prev-tag\"] = history[i-1]\n",
    "    return features\n",
    "\n",
    "class ConsecutivePosTagger(nltk.TaggerI):\n",
    "\n",
    "    def __init__(self, train_sents):\n",
    "        train_set = [] # 训练集\n",
    "        for tagged_sent in train_sents: # 提取训练文本中的句子\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent) # 去掉标注\n",
    "            history = [] # 标注集\n",
    "            for i, (word, tag) in enumerate(tagged_sent): # 对句子中的每个(序号, (词，词性))形式\n",
    "                featureset = pos_features(untagged_sent, i, history) # 对单词训练特征\n",
    "                train_set.append((featureset, tag)) # 向训练集中添加特征和标签\n",
    "                history.append(tag) # 向标注集中添加标注(此处的标注是真实值)\n",
    "        self.classifier = nltk.NaiveBayesClassifier.train(train_set) # 训练模型\n",
    "\n",
    "    def tag(self, sentence): # 测试集的标注过程\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = pos_features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset) # 对于每个单词，预测词性\n",
    "            history.append(tag) # 将预测结果添加到标注集里(此处的标注是预测值)\n",
    "        return zip(sentence, history)\n",
    "tagged_sents = brown.tagged_sents(categories='news')[:1000]\n",
    "size = int(len(tagged_sents) * 0.1)\n",
    "train_sents, test_sents = tagged_sents[size:], tagged_sents[:size]\n",
    "tagger = ConsecutivePosTagger(train_sents)\n",
    "print('the accuracy of train set {}'.format(tagger.evaluate(train_sents)))\n",
    "print('the accuracy of test set {}'.format(tagger.evaluate(test_sents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "序列分类存在一个问题，如果前面一个词性标注错了，后面的词性也很受影响。一种可能的解决方案是对所有可能的序列打分，选择最高分序列。\n",
    "\n",
    "隐形马尔可夫就采用了这种策略，它只看最近一个标记。也有更先进的方法，比如最大熵马尔可夫和条件随机场模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 有监督分类的更多例子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 句子分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = nltk.corpus.treebank_raw.sents()\n",
    "tokens = []\n",
    "boundaries = set()\n",
    "offset = 0\n",
    "for sent in sents:\n",
    "    tokens.extend(sent) # 获取句子内容\n",
    "    offset += len(sent)\n",
    "    boundaries.add(offset-1) # 找到句子边界"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy of train set 0.9699290250280165\n",
      "the accuracy of test set 0.936026936026936\n"
     ]
    }
   ],
   "source": [
    "def punct_features(tokens, i):\n",
    "    return {'next-word-capitalized': tokens[i+1][0].isupper(), # 下一个是否为大写\n",
    "            'prev-word': tokens[i-1].lower(), # 上一个单词是否为小写\n",
    "            'punct': tokens[i], # 是否标点符号\n",
    "            'prev-word-is-one-char': len(tokens[i-1]) == 1} # 前一个档次的长度是否为1\n",
    "featuresets = [(punct_features(tokens, i), (i in boundaries))\n",
    "               for i in range(1, len(tokens)-1)\n",
    "               if tokens[i] in '.?!']\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print('the accuracy of train set {}'.format(nltk.classify.accuracy(classifier, train_set)))\n",
    "print('the accuracy of test set {}'.format(nltk.classify.accuracy(classifier, test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 识别对话行为类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy of train set 0.6943333333333334\n",
      "the accuracy of test set 0.668\n"
     ]
    }
   ],
   "source": [
    "posts = nltk.corpus.nps_chat.xml_posts()[:10000]\n",
    "def dialogue_act_features(post):\n",
    "    features = {}\n",
    "    for word in nltk.word_tokenize(post):\n",
    "        features['contains({})'.format(word.lower())] = True\n",
    "    return features\n",
    "featuresets = [(dialogue_act_features(post.text), post.get('class'))\n",
    "               for post in posts]\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print('the accuracy of train set {}'.format(nltk.classify.accuracy(classifier, train_set)))\n",
    "print('the accuracy of test set {}'.format(nltk.classify.accuracy(classifier, test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 评估\n",
    "- 准确度\n",
    "- 精确度和召回率\n",
    "- 混淆矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 朴素贝叶斯分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 最大熵分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7 为语言模式建模"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
