{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 加工原料文本\n",
    "1. 我们怎样才能编写程序访问本地和网络上的文件，从而获得无限的语言材料？\n",
    "2. 我们如何把文档分割成单独的词和标点符号，这样我们就可以开始像前面章节中在文本语料上做的那样的分析？\n",
    "3. 我们怎样编程程序产生格式化的输出，并把结果保存在一个文件中？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 字符串：最底层的文本处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 字符串的基本操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monty Python\n",
      "================================================================================\n",
      "Monty Python's Flying Circus\n",
      "================================================================================\n",
      "Shall I compare thee to a Summer's day?Thou are more lovely and more temperate:\n",
      "================================================================================\n",
      "Shall I compare thee to a Summer's day?\n",
      "Thou are more lovely and more temperate:\n",
      "================================================================================\n",
      "veryveryvery\n"
     ]
    }
   ],
   "source": [
    "monty = 'Monty Python' # 基本字符串\n",
    "print(monty)\n",
    "print(\"================================================================================\")\n",
    "circus = 'Monty Python\\'s Flying Circus' # 转义字符\n",
    "print(circus)\n",
    "print(\"================================================================================\")\n",
    "couplet = \"Shall I compare thee to a Summer's day?\"\\\n",
    " \"Thou are more lovely and more temperate:\" # 多行\n",
    "print(couplet)\n",
    "print(\"================================================================================\")\n",
    "couplet2 = \"\"\"Shall I compare thee to a Summer's day?\n",
    "Thou are more lovely and more temperate:\"\"\" # 分行\n",
    "print(couplet2)\n",
    "print(\"================================================================================\")\n",
    "v = 'very' + 'very' + 'very' # 加号连接\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M\n",
      "n\n",
      "Monty\n"
     ]
    }
   ],
   "source": [
    "# 索引\n",
    "print(monty[0])\n",
    "print(monty[-1])\n",
    "print(monty[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 字符串匹配\n",
    "monty.find('Python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['m', 'o', 'b', 'y', 'd', 'i', 'c', 'k', 'h', 'e', 'r', 'a', 'n', 'l', 'v', 't', 'g', 's', 'u', 'p', 'w', 'x', 'q', 'f', 'j', 'z'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 字符计数\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "raw = gutenberg.raw('melville-moby_dick.txt')\n",
    "fdist = nltk.FreqDist(ch.lower() for ch in raw if ch.isalpha())\n",
    "fdist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John', 'Paul', 'George', 'Ringo']\n",
      "['me', 'Paul', 'George', 'Ringo']\n"
     ]
    }
   ],
   "source": [
    "# 链表与字符串差异\n",
    "query = 'Who knows?'\n",
    "beatles = ['John', 'Paul', 'George', 'Ringo']\n",
    "# 链表的可变性\n",
    "print(beatles)\n",
    "beatles[0] = 'me'\n",
    "print(beatles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 使用正则表达式检测词组搭配"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从文件中提取已编码文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['absorbed', 'abstracted', 'abstricted', 'accelerated', 'accepted', 'accidented']\n"
     ]
    }
   ],
   "source": [
    "# 以ed结尾的词\n",
    "print([w for w in wordlist if re.search('ed$', w)][10:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abjectly', 'adjuster', 'dejected', 'dejectly', 'injector', 'majestic', 'objectee', 'objector', 'rejecter', 'rejector', 'unjilted', 'unjolted', 'unjustly']\n"
     ]
    }
   ],
   "source": [
    "# 第3个字母是j，第6个字母是t，长度为8的单词\n",
    "print([w for w in wordlist if re.search('^..j..t..$', w)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gold', 'golf', 'hold', 'hole']\n"
     ]
    }
   ],
   "source": [
    "# 中括号[]表示其中字母的一个\n",
    "print([w for w in wordlist if re.search('^[ghi][mno][jlk][def]$', w)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['miiiiiiiiiiiiinnnnnnnnnnneeeeeeeeee', 'miiiiiinnnnnnnnnneeeeeeee', 'mine', 'mmmmmmmmiiiiiiiiinnnnnnnnneeeeeeee']\n"
     ]
    }
   ],
   "source": [
    "# +表示中间可以有表示前面的字符重复一次或以上；*代表零次或以上\n",
    "chat_words = sorted(set(w for w in nltk.corpus.nps_chat.words()))\n",
    "print([w for w in chat_words if re.search('^m+i+n+e+$', w)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.50', '0.54', '0.56', '0.60', '0.7', '0.82']\n"
     ]
    }
   ],
   "source": [
    "# 转移字符的使用\n",
    "wsj = sorted(set(nltk.corpus.treebank.words()))\n",
    "print([w for w in wsj if re.search('^[0-9]+\\.[0-9]+$', w)][10:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20-point', '20-stock', '21-month', '237-seat', '240-page', '27-year']\n"
     ]
    }
   ],
   "source": [
    "# {}大括号表示制定重复次数，可能为一个数字，也可能为一个上下限范围\n",
    "print([w for w in wsj if re.search('^[0-9]+-[a-z]{3,5}$', w)][10:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Arbitrage-related', 'Arbitraging', 'Asked', 'Assuming', 'Atlanta-based', 'Baking']\n"
     ]
    }
   ],
   "source": [
    "# 小括号(|)表示|前面或后面的内容\n",
    "print([w for w in wsj if re.search('(ed|ing)$', w)][10:16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用r'string'来表示正则表达式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 正则表达式的有益应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提取字符块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['u', 'e', 'a', 'i', 'a', 'i', 'i', 'i', 'e', 'i', 'a', 'i', 'o', 'i', 'o', 'u']\n"
     ]
    }
   ],
   "source": [
    "word = 'supercalifragilisticexpialidocious'\n",
    "print(re.findall(r'[aeiou]', word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ea': 476, 'oi': 65, 'ou': 329, 'io': 549, 'ee': 217, 'ie': 331, 'ui': 95, 'ua': 109, 'ai': 261, 'ue': 105, 'ia': 253, 'ei': 86, 'iai': 1, 'oo': 174, 'au': 106, 'eau': 10, 'oa': 59, 'oei': 1, 'oe': 15, 'eo': 39, 'uu': 1, 'eu': 18, 'iu': 14, 'aii': 1, 'aiia': 1, 'ae': 11, 'aa': 3, 'oui': 6, 'ieu': 3, 'ao': 6, 'iou': 27, 'uee': 4, 'eou': 5, 'aia': 1, 'uie': 3, 'iao': 1, 'eei': 2, 'uo': 8, 'uou': 5, 'eea': 1, 'ueui': 1, 'ioa': 1, 'ooi': 1}\n"
     ]
    }
   ],
   "source": [
    "# 使用正则表达式提取连续两个元音字母\n",
    "wsj = sorted(set(nltk.corpus.treebank.words()))\n",
    "fd = nltk.FreqDist(vs for word in wsj\n",
    "    for vs in re.findall(r'[aeiou]{2,}', word))\n",
    "print(dict(fd.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在字符块上做更多事情"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unvrsl Dclrtn of Hmn Rghts Prmble Whrs rcgntn of the inhrnt dgnty and\n",
      "of the eql and inlnble rghts of all mmbrs of the hmn fmly is the fndtn\n",
      "of frdm , jstce and pce in the wrld , Whrs dsrgrd and cntmpt fr hmn\n",
      "rghts hve rsltd in brbrs acts whch hve outrgd the cnscnce of mnknd ,\n",
      "and the advnt of a wrld in whch hmn bngs shll enjy frdm of spch and\n"
     ]
    }
   ],
   "source": [
    "# 保留以元音字母开始的、以元音字母结束的或没有元音字母的\n",
    "regexp = r'^[AEIOUaeiou]+|[AEIOUaeiou]+$|[^AEIOUaeiou]'\n",
    "def compress(word):\n",
    "    pieces = re.findall(regexp, word)\n",
    "    return ''.join(pieces)\n",
    "\n",
    "english_udhr = nltk.corpus.udhr.words('English-Latin1')\n",
    "print(nltk.tokenwrap(compress(w) for w in english_udhr[:75]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    a   e   i   o   u \n",
      "k 418 148  94 420 173 \n",
      "p  83  31 105  34  51 \n",
      "r 187  63  84  89  79 \n",
      "s   0   0 100   2   1 \n",
      "t  47   8   0 148  37 \n",
      "v  93  27 105  48  49 \n"
     ]
    }
   ],
   "source": [
    "# 包含ptksvr+元音字母的单词\n",
    "rotokas_words = nltk.corpus.toolbox.words('rotokas.dic')\n",
    "cvs = [cv for w in rotokas_words for cv in re.findall(r'[ptksvr][aeiou]', w)]\n",
    "cfd = nltk.ConditionalFreqDist(cvs)\n",
    "cfd.tabulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_word_pairs = [(cv, w) for w in rotokas_words for cv in re.findall(r'[ptksvr][aeiou]', w)]\n",
    "cv_index = nltk.Index(cv_word_pairs) # 对key重复的字典，将同一key下的所有values放到一个列表中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kaakaoko', 'kaakasi', 'kaakasi', 'kaakau', 'kaakau', 'kaakauko']\n"
     ]
    }
   ],
   "source": [
    "print(cv_index['ka'][10:16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查找词干"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ing']\n",
      "['processing']\n",
      "[('process', 'ing')]\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r'^.*(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')) # 找到词缀(如果添加括号，那么只显示括号中的部分)\n",
    "print(re.findall(r'^.*(?:ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')) # 找到整个单词，注意?:的形式\n",
    "print(re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')) # 找到词干和词缀(两个括号，会返回两个匹配)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('processe', 's')]\n",
      "[('process', 'es')]\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes')) # 贪婪查找，先尽可能匹配前面位置的规则\n",
    "print(re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes')) # 非贪婪查找，通过问号表示前面的内容是可选的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sword', 'i', 'no', 'basi', 'for', 'a']\n"
     ]
    }
   ],
   "source": [
    "def stem(word):\n",
    "    regexp = r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'\n",
    "    stem, suffix = re.findall(regexp, word)[0]\n",
    "    return stem\n",
    "\n",
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "    is no basis for a system of government. Supreme executive power derives from\n",
    "    a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "print([stem(t) for t in tokens][10:16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搜索已分词文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monied; nervous; dangerous; white; white; white; pious; queer; good;\n",
      "mature; white; Cape; great; wise; wise; butterless; white; fiendish;\n",
      "pale; furious; better; certain; complete; dismasted; younger; brave;\n",
      "brave; brave; brave\n"
     ]
    }
   ],
   "source": [
    "# 匹配两个<>中间的单词部分(如果添加括号，那么只显示括号中的部分)\n",
    "from nltk.corpus import gutenberg, nps_chat\n",
    "moby = nltk.Text(gutenberg.words('melville-moby_dick.txt'))\n",
    "moby.findall(r\"<a> (<.*>) <man>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you rule bro; telling you bro; u twizted bro\n"
     ]
    }
   ],
   "source": [
    "# 匹配<>单词前的两个字符\n",
    "chat = nltk.Text(nps_chat.words())\n",
    "chat.findall(r\"<.*> <.*> <bro>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lol lol lol; lmao lol lol; lol lol lol; la la la la la; la la la; la\n",
      "la la; lovely lol lol love; lol lol lol.; la la la; la la la\n"
     ]
    }
   ],
   "source": [
    "# 连续三个以上符合形式的单词\n",
    "chat.findall(r\"<l.*>{3,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speed and other activities; water and other liquids; tomb and other\n",
      "landmarks; Statues and other monuments; pearls and other jewels;\n",
      "charts and other items; roads and other features; figures and other\n",
      "objects; military and other areas; demands and other factors;\n",
      "abstracts and other compilations; iron and other metals\n"
     ]
    }
   ],
   "source": [
    "# 查找X and other Ys形式\n",
    "from nltk.corpus import brown\n",
    "hobbies_learned = nltk.Text(brown.words(categories=['hobbies', 'learned']))\n",
    "hobbies_learned.findall(r\"<\\w*> <and> <other> <\\w*s>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 规范化文本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词干提取器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "    is no basis for a system of government. Supreme executive power derives from\n",
    "    a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "# 两种词干提取器\n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['denni', ':', 'listen', ',', 'strang', 'women', 'lie', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'power', 'deriv', 'from', 'a', 'mandat', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcic', 'aquat', 'ceremoni', '.']\n",
      "['den', ':', 'list', ',', 'strange', 'wom', 'lying', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'bas', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'pow', 'der', 'from', 'a', 'mand', 'from', 'the', 'mass', ',', 'not', 'from', 'som', 'farc', 'aqu', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "print([porter.stem(t) for t in tokens])\n",
    "print([lancaster.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sten , strange women lying in ponds distr\n",
      "etreat . ROBIN : All lies ! MINSTREL : [ \n",
      "    . Come . You may lie here . Oh ,     \n",
      "     , no , please ! Lie down . [ clap   \n",
      " for beyond the cave lies the Gorge of Et\n",
      ": To the north there lies a cave -- the  \n",
      "es of full fifty men lie strewn about its\n",
      " til each one of you lies dead , and the \n"
     ]
    }
   ],
   "source": [
    "# 索引上下文本\n",
    "class IndexedText(object):\n",
    "\n",
    "    def __init__(self, stemmer, text):\n",
    "        self._text = text # 文本内容\n",
    "        self._stemmer = stemmer # 使用的词干提取器\n",
    "        self._index = nltk.Index((self._stem(word), i)\n",
    "                                 for (i, word) in enumerate(text)) # 生成(词干,索引)的组合\n",
    "\n",
    "    def concordance(self, word, width=40): # 找到上下文函数：需要指定单词和上下文宽度\n",
    "        key = self._stem(word) # key为单词经过词干提取后的形式\n",
    "        wc = int(width/4)                # 上下文的词数\n",
    "        for i in self._index[key]: # 尽管单词形式可以不同，但只要满足词干相同，就会被匹配到\n",
    "            lcontext = ' '.join(self._text[i-wc:i])\n",
    "            rcontext = ' '.join(self._text[i:i+wc])\n",
    "            ldisplay = '{:>{width}}'.format(lcontext[-width:], width=width)\n",
    "            rdisplay = '{:{width}}'.format(rcontext[:width], width=width)\n",
    "            print(ldisplay, rdisplay)\n",
    "\n",
    "    def _stem(self, word): # 词干提取函数(并转化成小写)\n",
    "        return self._stemmer.stem(word).lower()\n",
    "porter = nltk.PorterStemmer()\n",
    "grail = nltk.corpus.webtext.words('grail.txt')\n",
    "text = IndexedText(porter, grail)\n",
    "text.concordance('lie', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词形归并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DENNIS', 'DENNIS'),\n",
       " (':', ':'),\n",
       " ('Listen', 'Listen'),\n",
       " (',', ','),\n",
       " ('strange', 'strange'),\n",
       " ('woman', 'women'),\n",
       " ('lying', 'lying'),\n",
       " ('in', 'in'),\n",
       " ('pond', 'ponds'),\n",
       " ('distributing', 'distributing')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 词性冰柜不同于词干提取，它会展示词在字典中的出现形式。例如lying因为在字典中存在所以不会被处理，但名词复数会被还原为单数\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "[(wnl.lemmatize(t),t) for t in tokens][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 用正则表达式为文本分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词的简单方法(匹配或分割)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'When\", \"I'M\", 'a', \"Duchess,'\", 'she', 'said', 'to', 'herself,', '(not', 'in', 'a', 'very', 'hopeful', 'tone', 'though),', \"'I\", \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL.', 'Soup', 'does', 'very', 'well', 'without--Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', \"hot-tempered,'...\"]\n"
     ]
    }
   ],
   "source": [
    "# 利用分隔符的简单方法\n",
    "raw = \"\"\"'When I'M a Duchess,' she said to herself, (not in a very hopeful tone\n",
    "    though), 'I won't have any pepper in my kitchen AT ALL. Soup does very\n",
    "    well without--Maybe it's always pepper that makes people hot-tempered,'...\"\"\"\n",
    "print(re.split(r'[ :\\t\\n]+', raw)) # 输入分隔符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'When', 'I', 'M', 'a', 'Duchess', 'she', 'said', 'to', 'herself', 'not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', 'I', 'won', 't', 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', 'Soup', 'does', 'very', 'well', 'without', 'Maybe', 'it', 's', 'always', 'pepper', 'that', 'makes', 'people', 'hot', 'tempered', '']\n"
     ]
    }
   ],
   "source": [
    "print(re.split(r'\\W+', raw)) # 排除所有非字母形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'When\", 'I', \"'M\", 'a', 'Duchess', ',', \"'\", 'she', 'said', 'to', 'herself', ',', '(not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', ')', ',', \"'I\", 'won', \"'t\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', '.', 'Soup', 'does', 'very', 'well', 'without', '-', '-Maybe', 'it', \"'s\", 'always', 'pepper', 'that', 'makes', 'people', 'hot', '-tempered', ',', \"'\", '.', '.', '.']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r'\\w+|\\S\\w*', raw)) # 找到全部为字母，或者以非空白形式开头+字母的形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'\", 'When', \"I'M\", 'a', 'Duchess', ',', \"'\", 'she', 'said', 'to', 'herself', ',', '(', 'not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', ')', ',', \"'\", 'I', \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', '.', 'Soup', 'does', 'very', 'well', 'without', '--', 'Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', 'hot-tempered', ',', \"'\", '...']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r\"\\w+(?:[-']\\w+)*|'|[-.(]+|\\S\\w*\", raw)) # 匹配连接符形式、'、多个字符、或者以非空白形式开头+字母的形式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK 的正则表达式分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'\", 'When', \"I'M\", 'a', 'Duchess', ',', \"'\", 'she', 'said', 'to', 'herself', ',', '(', 'not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', ')', ',', \"'\", 'I', \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', '.', 'Soup', 'does', 'very', 'well', 'without', '--', 'Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', 'hot-tempered', ',', \"'\", '...']\n"
     ]
    }
   ],
   "source": [
    "pattern = r\"\\w+(?:[-']\\w+)*|'|[-.(]+|\\S\\w*\"\n",
    "print(nltk.regexp_tokenize(raw, pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 分割"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 断句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Nonsense!\"\n",
      "said Gregory, who was very rational when anyone else\n",
      "attempted paradox.\n",
      "\"Why do all the clerks and navvies in the\n",
      "railway trains look so sad and tired, so very sad and tired?\n",
      "I will\n",
      "tell you.\n",
      "It is because they know that the train is going right.\n",
      "It\n",
      "is because they know that whatever place they have taken a ticket\n",
      "for that place they will reach.\n"
     ]
    }
   ],
   "source": [
    "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "text = nltk.corpus.gutenberg.raw('chesterton-thursday.txt')\n",
    "sents = sent_tokenizer.tokenize(text)\n",
    "for i in sents[79:85]: \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 ['doyo', 'useet', 'hek', 'itty', 'seeth', 'edoggy', 'doyouliket', 'hek', 'itty', 'liketh', 'edoggy']\n",
      "59 ['doyou', 'seet', 'he', 'k', 'itty', 'seethedoggy', 'doyou', 'likethe', 'k', 'itty', 'likethe', 'doggy']\n",
      "57 ['do', 'you', 'seet', 'he', 'k', 'itty', 'seet', 'he', 'doggy', 'doyou', 'liket', 'he', 'k', 'itty', 'liket', 'he', 'doggy']\n",
      "55 ['doy', 'ou', 'seet', 'he', 'k', 'itty', 'seet', 'he', 'doggy', 'doy', 'ou', 'l', 'iket', 'he', 'k', 'itty', 'l', 'iket', 'he', 'doggy']\n",
      "52 ['do', 'you', 'seet', 'he', 'k', 'itty', 'seet', 'he', 'doggy', 'do', 'you', 'liket', 'he', 'k', 'itty', 'liket', 'he', 'doggy']\n",
      "49 ['do', 'you', 'seet', 'he', 'kitty', 'seet', 'he', 'doggy', 'do', 'you', 'liket', 'he', 'kitty', 'liket', 'he', 'doggy']\n",
      "46 ['doyou', 'seet', 'he', 'kitty', 'seet', 'he', 'doggy', 'doyou', 'liket', 'he', 'kitty', 'liket', 'he', 'doggy']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0000100010100001000101000010000100001010000100001010000'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分割字符\n",
    "def segment(text, segs):\n",
    "    words = []\n",
    "    last = 0\n",
    "    for i in range(len(segs)):\n",
    "        if segs[i] == '1':\n",
    "            words.append(text[last:i+1])\n",
    "            last = i+1\n",
    "    words.append(text[last:])\n",
    "    return words\n",
    "\n",
    "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
    "seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
    "seg2 = \"0100100100100001001001000010100100010010000100010010000\"\n",
    "seg3 = \"0000100100000011001000000110000100010000001100010000001\"\n",
    "segment(text, seg1)\n",
    "\n",
    "# 评估指标\n",
    "def evaluate(text, segs):\n",
    "    words = segment(text, segs)\n",
    "    text_size = len(words)\n",
    "    lexicon_size = sum(len(word) + 1 for word in set(words))\n",
    "    return text_size + lexicon_size\n",
    "evaluate(text, seg1)\n",
    "\n",
    "# 模拟退火算法\n",
    "from random import randint\n",
    "\n",
    "# 更换一个位置的0-1值\n",
    "def flip(segs, pos):\n",
    "    return segs[:pos] + str(1-int(segs[pos])) + segs[pos+1:]\n",
    "\n",
    "# 更换n个位置的0-1值，其中n由当前温度决定，更换的位置随机的\n",
    "def flip_n(segs, n):\n",
    "    for i in range(n):\n",
    "        segs = flip(segs, randint(0, len(segs)-1))\n",
    "    return segs\n",
    "\n",
    "# 退火模拟算法\n",
    "def anneal(text, segs, iterations, cooling_rate): \n",
    "    '''\n",
    "    text: 文本内容\n",
    "    segs: 字位串, 长度为len(text)-1\n",
    "    iterations: 每个温度下的迭代次数\n",
    "    cooling_rate: 冷却速率, 越大温度下降越快, 模拟轮数也就越少\n",
    "    '''\n",
    "    temperature = float(len(segs)) # 初始温度为字位串的长度\n",
    "    while temperature > 0.5: \n",
    "        best_segs, best = segs, evaluate(text, segs) # 当前最佳分割法\n",
    "        for i in range(iterations): # 在当前温度下开始迭代\n",
    "            guess = flip_n(segs, round(temperature)) # 随机翻转几个位置，进行猜测\n",
    "            score = evaluate(text, guess) # 检验猜测结果\n",
    "            if score < best:\n",
    "                best, best_segs = score, guess # 如果效果提升，用本次猜测结果来替代最佳结果\n",
    "                print(evaluate(text, best_segs), segment(text, best_segs))\n",
    "        score, segs = best, best_segs # 当前温度下的迭代结束，更新最佳得分与字位串\n",
    "        temperature = temperature / cooling_rate # 冷却，并准备下一轮迭代\n",
    "    return segs\n",
    "anneal(text, seg1, 5000, 1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9 格式化：从链表到字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We called him Tortoise because he taught us .\n"
     ]
    }
   ],
   "source": [
    "# 从链表到字符串\n",
    "silly = ['We', 'called', 'him', 'Tortoise', 'because', 'he', 'taught', 'us', '.']\n",
    "print(' '.join(silly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat -> 3; dog -> 4; snake -> 1;  \n",
      "cat->3; dog->4; snake->1; "
     ]
    }
   ],
   "source": [
    "# 字符串格式化表达式\n",
    "fdist = nltk.FreqDist(['dog', 'cat', 'dog', 'cat', 'dog', 'snake', 'dog', 'cat'])\n",
    "for word in sorted(fdist):\n",
    "    print(word, '->', fdist[word], end='; ') # 原始方法\n",
    "print(' ')\n",
    "for word in sorted(fdist):\n",
    "    print('{}->{};'.format(word, fdist[word]), end=' ') # format形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from B to A\n",
      "  41\n",
      "accuracy for 9375 words: 34.1867%\n"
     ]
    }
   ],
   "source": [
    "print('from {1} to {0}'.format('A', 'B')) # 设置索引\n",
    "print('{0:4}'.format(41)) # 设置宽度\n",
    "count, total = 3205, 9375\n",
    "print(\"accuracy for {} words: {:.4%}\".format(total, count / total)) # 设置数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 写入csv\n",
    "output_file = open('output.txt', 'w')\n",
    "words = set(nltk.corpus.genesis.words('english-kjv.txt'))\n",
    "for word in sorted(words):\n",
    "    print(word, file=output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After (5), all (3), is (2), said (4), and (3), done (4), , (1), more (4), is (2), said (4), than (4), done (4), . (1),\n",
      "After (5), all (3), is (2), said (4), and (3), done (4), , (1), more\n",
      "(4), is (2), said (4), than (4), done (4), . (1),\n"
     ]
    }
   ],
   "source": [
    "# 文本换行\n",
    "from textwrap import fill\n",
    "saying = ['After', 'all', 'is', 'said', 'and', 'done', ',',\n",
    "            'more', 'is', 'said', 'than', 'done', '.']\n",
    "format = '%s (%d),'\n",
    "pieces = [format % (word, len(word)) for word in saying]\n",
    "output = ' '.join(pieces)\n",
    "print(output) # 直接输出\n",
    "wrapped = fill(output)\n",
    "print(wrapped) # 换行输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category            can  could    may  might   must   will \n",
      "news                 93     86     66     38     50    389 \n",
      "religion             82     59     78     12     54     71 \n",
      "hobbies             268     58    131     22     83    264 \n",
      "science_fiction      16     49      4     12      8     16 \n",
      "romance              74    193     11     51     45     43 \n",
      "humor                16     30      8      8      9     13 \n"
     ]
    }
   ],
   "source": [
    "# 结构化输出\n",
    "def tabulate(cfdist, words, categories):\n",
    "    print('{:16}'.format('Category'), end=' ')                    # 第一列的列名\n",
    "    for word in words:\n",
    "        print('{:>6}'.format(word), end=' ')                      # 其他列的列名\n",
    "    print()\n",
    "    for category in categories:\n",
    "        print('{:16}'.format(category), end=' ')                  # 每一行的行名\n",
    "        for word in words:                                        \n",
    "            print('{:6}'.format(cfdist[category][word]), end=' ') # 每一行的值\n",
    "        print()                                                   \n",
    "from nltk.corpus import brown\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "           (genre, word)\n",
    "           for genre in brown.categories()\n",
    "           for word in brown.words(categories=genre))\n",
    "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "tabulate(cfd, modals, genres)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
